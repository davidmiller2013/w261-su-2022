{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "228f0da8-cb3c-41ca-839f-9afd84c7b7cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo 5 - Accumulators, Aggregations, and Joins in Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "\n",
    "By the end of this demo you should be able to: \n",
    "* ... __implement__ a custom accumulator\n",
    "* ... __explain__ different types of aggregations and how they are implemented in Spark.\n",
    "* ... __explain__ how different join operations are implemented in Spark\n",
    "* ... __explain__  the challenges of implementing the A Priori algorithm at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51c8aa6b-46e2-4c9d-87ce-de711ba8c05c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "270a0bb5-3f7b-48e8-849b-e6778d36285a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run the next three cells to create your DEMO5 directory \n",
    "The scala code below fetches your username automatically and creates a temporary Spark table that can be read by python in the following cell. Don't worry about understanding this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24795c11-d319-43b1-b68d-9b1daa7103b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">dbfs:/user/kylehamilton@ischool.berkeley.edu\n",
       "Out[7]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">dbfs:/user/kylehamilton@ischool.berkeley.edu\nOut[7]: True</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN THIS CELL AS IS\n",
    "# This code snippet reads the user directory name, and stores is in a python variable.\n",
    "# Next, it creates a folder inside your home folder, which you will use for files which you save inside this notebook.\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "userhome = 'dbfs:/user/' + username\n",
    "print(userhome)\n",
    "demo5_path = userhome + \"/demo5/\" \n",
    "demo5_path_open = '/dbfs' + demo5_path.split(':')[-1] # for use with python open()\n",
    "dbutils.fs.mkdirs(demo5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37f2faa3-b289-4fa9-b1c9-12db5c05a782",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Wrote 11 bytes.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Wrote 11 bytes.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/kylehamilton@ischool.berkeley.edu/demo5/test5.txt</td><td>test5.txt</td><td>11</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/kylehamilton@ischool.berkeley.edu/demo5/test5.txt",
         "test5.txt",
         11
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": [],
        "xColumns": [],
        "yColumns": []
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN THIS CELL AS IS\n",
    "# Here we'll create a test file, and use databricks utils to makes usre everything works as expected.\n",
    "# You should see a result like: dbfs:/user/<your email>@ischool.berkeley.edu/demo4/test.txt\n",
    "dbutils.fs.put(demo5_path+'test5.txt',\"hello world\",True)\n",
    "display(dbutils.fs.ls(demo5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ceb0c704-6c2f-44c7-bae6-c6a614cfa556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a55badb8-67a5-425e-9a15-22844006eee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82803f04-3228-4365-afac-992ac04ff248",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Accumulators\n",
    "Definitive Guide book, pg. 241"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b123b51-6e2a-4d1d-947f-e6d439ee9bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Accumulators are Spark's equivalent of Hadoop counters. Like broadcast variables they represent shared information across the nodes in your cluster, but unlike broadcast variables accumulators are _write-only_ ... in other words you can only access their values in the driver program and not on your executors (where transformations are applied). As convenient as this sounds, there are a few common pitfalls to avoid. Let's take a look.\n",
    "\n",
    "Run the following cell to create a sample data file representing a list of `studentID, courseID, final_grade`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5bf8777-e55c-4d13-aee4-5f9e3adca34e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b60e8a2e-64ba-4f4d-8d76-5c375a310387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Wrote 129 bytes.\n",
       "Out[11]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Wrote 129 bytes.\nOut[11]: True</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.put(demo5_path+\"grades.csv\", \n",
    "\"\"\"10001,101,98\n",
    "10001,102,87\n",
    "10002,101,75\n",
    "10002,102,55\n",
    "10002,103,80\n",
    "10003,102,45\n",
    "10003,103,75\n",
    "10004,101,90\n",
    "10005,101,85\n",
    "10005,103,60\"\"\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf49b88d-695c-4a56-bed2-d01f1021edaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Suppose we want to compute the average grade by course and student while also tracking the number of failing grades awarded. We might try something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b369247-31c1-489d-9404-aa6652b0973f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to increment the accumulator as we read in the data\n",
    "def parse_grades(line, accumulator):\n",
    "    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n",
    "    student,course,grade = line.split(',')\n",
    "    grade = int(grade)\n",
    "    if grade < 65:\n",
    "        accumulator.add(1)\n",
    "    return(student,course, grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20b6ef63-176c-4896-b077-afd8184216fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize an accumulator to track failing grades\n",
    "nFailing = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80419fc8-3d92-4f52-878e-49e74fc31f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute averages in spark\n",
    "nFailing = sc.accumulator(0)\n",
    "gradesRDD = sc.textFile(demo5_path+'grades.csv')\\\n",
    "              .map(lambda x: parse_grades(x, nFailing))\n",
    "\n",
    "gradesRDD.cache()\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                       .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "\n",
    "\n",
    "courseAvgs = gradesRDD.map(lambda x: (x[1], (x[2], 1)))\\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                      .mapValues(lambda x: x[0]/x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb031a71-ccc3-4e91-bf9f-9b86c5042df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">===== average by student =====\n",
       "[(&#39;10001&#39;, 92.5), (&#39;10004&#39;, 90.0), (&#39;10003&#39;, 60.0), (&#39;10005&#39;, 72.5), (&#39;10002&#39;, 70.0)]\n",
       "===== average by course =====\n",
       "[(&#39;102&#39;, 62.333333333333336), (&#39;103&#39;, 71.66666666666667), (&#39;101&#39;, 87.0)]\n",
       "===== number of failing grades awarded =====\n",
       "3\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">===== average by student =====\n[(&#39;10001&#39;, 92.5), (&#39;10004&#39;, 90.0), (&#39;10003&#39;, 60.0), (&#39;10005&#39;, 72.5), (&#39;10002&#39;, 70.0)]\n===== average by course =====\n[(&#39;102&#39;, 62.333333333333336), (&#39;103&#39;, 71.66666666666667), (&#39;101&#39;, 87.0)]\n===== number of failing grades awarded =====\n3\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a look\n",
    "print(\"===== average by student =====\")\n",
    "print(studentAvgs.collect())\n",
    "print(\"===== average by course =====\")\n",
    "print(courseAvgs.collect())\n",
    "print(\"===== number of failing grades awarded =====\")\n",
    "print(nFailing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5216fcdd-eabd-4769-a50b-5595f02e1348",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">===== average by student =====\n",
       "[(&#39;10001&#39;, 92.5), (&#39;10004&#39;, 90.0), (&#39;10003&#39;, 60.0), (&#39;10005&#39;, 72.5), (&#39;10002&#39;, 70.0)]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">===== average by student =====\n[(&#39;10001&#39;, 92.5), (&#39;10004&#39;, 90.0), (&#39;10003&#39;, 60.0), (&#39;10005&#39;, 72.5), (&#39;10002&#39;, 70.0)]\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"===== average by student =====\")\n",
    "print(studentAvgs.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e38a6060-c8ed-46d0-8b0a-30bbf5849828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">===== number of failing grades awarded =====\n",
       "3\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">===== number of failing grades awarded =====\n3\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"===== number of failing grades awarded =====\")\n",
    "print(nFailing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ff315c1-fe55-434e-8b08-13bbcd3c419e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# compute averages in spark\n",
    "# initialize an accumulator to track failing grades\n",
    "nFailing = sc.accumulator(0)\n",
    "\n",
    "def parse_grades(line):\n",
    "    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n",
    "    student,course,grade = line.split(',')\n",
    "    grade = int(grade)\n",
    "    return(student,course, grade)\n",
    "  \n",
    "def accAgg(row):\n",
    "    grade = row[2]\n",
    "    if grade < 65:\n",
    "        nFailing.add(1)\n",
    "\n",
    "gradesRDD = sc.textFile(demo5_path+'grades.csv')\\\n",
    "              .map(lambda x: parse_grades(x))\n",
    "\n",
    "gradesRDD.cache()\n",
    "\n",
    "gradesRDD.foreach(accAgg)\n",
    "\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                       .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "courseAvgs = gradesRDD.map(lambda x: (x[1], (x[2], 1)))\\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                      .mapValues(lambda x: x[0]/x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d78cb1dd-1a15-4712-bc25-ce16475d66bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "> __DISCUSSION QUESTIONS:__\n",
    "* What is wrong with the results? (__`HINT:`__ _how many failing grades are there really?_)\n",
    "* Why might this be happening? (__`HINT:`__ _How many actions are there in this code? Which parts of the DAG are recomputed for each of these actions?_)\n",
    "* What one line could we add to the code to fix this problem?\n",
    "  * What could go wrong with our \"fix\"?\n",
    "* How could we have designed our parser differently to avoid this problem in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce2e4ffa-0ee6-45cf-bb40-34a5fb72b3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom Accumulators\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "637fca73-df7e-4044-944e-a69fe4a1d1d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "While SparkContext supports accumulators for primitive data types like int and float, users can also define accumulators for custom types by providing a custom AccumulatorParam object. \n",
    "\n",
    "We may want to utilize custom accumulators later in the course when we implement PageRank, or Shortest Path (graph) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04fed041-b398-47bb-8819-a62339cb85cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# Spark only implements Accumulator parameter for numeric types.\n",
    "# This class extends Accumulator support to the string type.\n",
    "class StringAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 +\" -> \"+ val2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e4cabfbc-bf3c-4721-a122-79e3a9df1c53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's switch gears now for a moment, and look at aggregations. We'll come back to our string accumulator later in the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "040fdf36-f9e2-45e9-b06c-5171bdad7546",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1457c48f-2515-4320-84c2-8968702fadef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## groupByKey()\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.groupByKey   \n",
    "Easy to reason about, because it's very familiar folks coming from the SQL world. However, for the majority of cases, this is the wrong approach. The fundamental issue here is that each executor must hold all values for a given key in memory before applying the function to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2f4ddc1-9d29-4a93-b3f1-471cdde47015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/groupbykey.png?raw=true\" width=\"100%\"/>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/groupbykey.png?raw=true\" width=\"100%\"/>",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML('<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/groupbykey.png?raw=true\" width=\"100%\"/>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab464ef3-3c49-49eb-80a4-19d0f4d28d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## reduceByKey(Func)\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.reduceByKey    \n",
    "A much more stable approach to additive problems is reduceByKey. This is because the reduce happens within each partition and doesn’t need to put everything in memory. Additionally, there is no incurred shuffle during this operation; everything happens at each worker individually before performing the final reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff19e739-aade-4b14-add3-9ba3d820111e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### IMPORTANT NOTE ABOUT reduceByKey() and reduce()\n",
    "\n",
    "`reduceByKey()` is a transformation, whereas `reduce()` includes an action!\n",
    "Looking at the source code for reduce(), notice the `collect()` \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa7a3d54-f664-47b5-bbca-a129f4c71123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/reducebykey.png?raw=true\" width=\"50%\"/>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/reducebykey.png?raw=true\" width=\"50%\"/>",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML('<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/reducebykey.png?raw=true\" width=\"50%\"/>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80b8e12f-7a2a-4d27-9b35-443c611afa9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.combineByKey   \n",
    "The first function input to the combiner specifies how to merge values, and the second function specifies how to merge combiners. For example, we might want to add values to a list, and subsequently merge the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf8f78ff-6886-415c-9975-e929e37f2f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/combinebykey.png?raw=true\" width=\"50%\"/>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/combinebykey.png?raw=true\" width=\"50%\"/>",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML('<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/combinebykey.png?raw=true\" width=\"50%\"/>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd3e5821-9c73-42d2-87b3-ad763eee8f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## foldByKey(zeroValue, Func)\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.foldByKey   \n",
    "Calls combineByKey, but allows us to use a zero value which can be added to the result an arbitrary number of\n",
    "times, and must not change the result (eg. 0 for addition, 1 for multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70917d27-faaa-4592-b1e5-a80dd51bfcaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## aggregateByKey(zeroValue, seqOp, combOp)\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f48d1cfc-cce1-41e1-920c-19413287a300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/aggregatebykey.png?raw=true\" width=\"100%\"/>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/aggregatebykey.png?raw=true\" width=\"100%\"/>",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML('<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/aggregatebykey.png?raw=true\" width=\"100%\"/>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f637e5c3-f751-483d-a5b0-05f977880ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## treeAggregate(zeroValue, seqOp, combOp, depth)\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.treeAggregate    \n",
    "Same as aggregate except it “pushes down” some of the subaggregations (creating a tree from executor to executor)\n",
    "before performing \u001fnal aggregations on the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "475ed5b2-7460-4f72-8c9f-1b68800b591f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00c57f9d-5a2d-4245-9f30-1685892b73a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Back to our example: \n",
    "What if we wanted to get a list of letter grades that each student recieved as well as their average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17f7405a-a8d8-4338-b632-235985bc45c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__INSTRUCTOR NOTES__   \n",
    "The following code block is going to fail. The objective is to help students learn debugging. We'll also pay homage to Bob Ross. While designing this exercise, your lecturer, Kyle Hamilton, made some mistakes. But just like Bob, Kyle doesn't believe in mistakes, only happy accidents!!\n",
    "\n",
    "Some questions to ask:   \n",
    "\n",
    "Q: What does this error message tell us?   \n",
    "A: list index out of bounds   \n",
    "\n",
    "Q: How can we go about debugging the problem?   \n",
    "A: Run each function to inspect the output. Is it what we are expecting? Does this make sense of the outofbounds error?    \n",
    "What's odd about the output from the reduceByKey?   \n",
    "Look at the data, and observe that student 10004 has only one entry. Looks like Spark did not run the reduceByKey function because it wasn't needed. That's a double edged sword! Yes, it's efficient, but oops, it made our code break. Takeaway: the signature of the input to the reducer must match the signature of the output. Where did we see this before?   \n",
    "\n",
    "Q: What other aggregation functions can we use here to solve this problem?   \n",
    "A: Some things to try:\n",
    "1. FoldByKey - we get a default value to start with   \n",
    "2. CombineByKey - we have more control over how data is combined and reduced. Why didn't this work? reduceByKey calls combineByKey. We're performing the same operation as before, so this doesn't help fix our problem\n",
    "3. AggregateByKey - allows us to specify a null value, forcing the evaluation of the reduce step. How is this different from foldByKey?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ad1226b-b7eb-438b-acb3-9e8cac587fec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40418c1e-9508-434e-9218-30fc0122acd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3829056957480028&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     16</span> studentAvgs <span class=\"ansi-blue-fg\">=</span> gradesRDD<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     17</span>                        <span class=\"ansi-blue-fg\">.</span>reduceByKey<span class=\"ansi-blue-fg\">(</span>getCounts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 18</span><span class=\"ansi-red-fg\">                        </span><span class=\"ansi-blue-fg\">.</span>mapValues<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">/</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>                        <span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n",
       "<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 34, 10.109.247.129, executor 3): org.apache.spark.api.python.PythonException: &#39;IndexError: tuple index out of range&#39;, from &lt;command-3829056957480028&gt;, line 18. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n",
       "    process()\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2099, in &lt;lambda&gt;\n",
       "    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n",
       "  File &#34;&lt;command-3829056957480028&gt;&#34;, line 18, in &lt;lambda&gt;\n",
       "IndexError: tuple index out of range\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2373)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2354)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2373)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n",
       "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.api.python.PythonException: &#39;IndexError: tuple index out of range&#39;, from &lt;command-3829056957480028&gt;, line 18. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n",
       "    process()\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2099, in &lt;lambda&gt;\n",
       "    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n",
       "  File &#34;&lt;command-3829056957480028&gt;&#34;, line 18, in &lt;lambda&gt;\n",
       "IndexError: tuple index out of range\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2373)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3829056957480028&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span> studentAvgs <span class=\"ansi-blue-fg\">=</span> gradesRDD<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span>                        <span class=\"ansi-blue-fg\">.</span>reduceByKey<span class=\"ansi-blue-fg\">(</span>getCounts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">---&gt; 18</span><span class=\"ansi-red-fg\">                        </span><span class=\"ansi-blue-fg\">.</span>mapValues<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">/</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>                        <span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 34, 10.109.247.129, executor 3): org.apache.spark.api.python.PythonException: &#39;IndexError: tuple index out of range&#39;, from &lt;command-3829056957480028&gt;, line 18. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2099, in &lt;lambda&gt;\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File &#34;&lt;command-3829056957480028&gt;&#34;, line 18, in &lt;lambda&gt;\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2373)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2354)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2373)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;IndexError: tuple index out of range&#39;, from &lt;command-3829056957480028&gt;, line 18. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 676, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 668, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 110, in wrapper\n    return f(*args, **kwargs)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2099, in &lt;lambda&gt;\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File &#34;&lt;command-3829056957480028&gt;&#34;, line 18, in &lt;lambda&gt;\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:603)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:738)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:721)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:556)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2373)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$8(Executor.scala:677)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 34, 10.109.247.129, executor 3): org.apache.spark.api.python.PythonException: &#39;IndexError: tuple index out of range&#39;, from &lt;command-3829056957480028&gt;, line 18. Full traceback below:",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toLetterGrade(x):\n",
    "    if x > 92.0:\n",
    "        return \"A\"\n",
    "    elif x > 82.0:\n",
    "        return \"B\"\n",
    "    elif x > 72.0:\n",
    "        return \"C\"\n",
    "    elif x > 65.0:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "def getCounts(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]))\n",
    "    \n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(getCounts)\\\n",
    "                       .mapValues(lambda x: ((x[0]/x[1]),x[2]))\\\n",
    "                       .collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e727f0e3-48e0-4a40-b9d5-c6992d06a249",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How can we debug this problem?\n",
    "1. What is the error?\n",
    "2. Insert as many cells as you need to figure out what happened. Next we'll look at some ways to \"fix\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33f1579f-26b9-4a7c-bd9c-17e6465ed7f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/Bob-Ross-3.jpg?raw=true' width=50%/>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/Bob-Ross-3.jpg?raw=true' width=50%/>",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML(\"<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/Bob-Ross-3.jpg?raw=true' width=50%/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c102381-0f14-4b47-b56f-d6fcfa26a7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[23]: [(&#39;10001&#39;, (98, 1)),\n",
       " (&#39;10001&#39;, (87, 1)),\n",
       " (&#39;10002&#39;, (75, 1)),\n",
       " (&#39;10002&#39;, (55, 1)),\n",
       " (&#39;10002&#39;, (80, 1)),\n",
       " (&#39;10003&#39;, (45, 1)),\n",
       " (&#39;10003&#39;, (75, 1)),\n",
       " (&#39;10004&#39;, (90, 1)),\n",
       " (&#39;10005&#39;, (85, 1)),\n",
       " (&#39;10005&#39;, (60, 1))]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[23]: [(&#39;10001&#39;, (98, 1)),\n (&#39;10001&#39;, (87, 1)),\n (&#39;10002&#39;, (75, 1)),\n (&#39;10002&#39;, (55, 1)),\n (&#39;10002&#39;, (80, 1)),\n (&#39;10003&#39;, (45, 1)),\n (&#39;10003&#39;, (75, 1)),\n (&#39;10004&#39;, (90, 1)),\n (&#39;10005&#39;, (85, 1)),\n (&#39;10005&#39;, (60, 1))]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d89f5a89-66ef-4b7d-9686-a9b6354dd341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[25]: [(&#39;10001&#39;, &#39;101&#39;, 98),\n",
       " (&#39;10001&#39;, &#39;102&#39;, 87),\n",
       " (&#39;10002&#39;, &#39;101&#39;, 75),\n",
       " (&#39;10002&#39;, &#39;102&#39;, 55),\n",
       " (&#39;10002&#39;, &#39;103&#39;, 80),\n",
       " (&#39;10003&#39;, &#39;102&#39;, 45),\n",
       " (&#39;10003&#39;, &#39;103&#39;, 75),\n",
       " (&#39;10004&#39;, &#39;101&#39;, 90),\n",
       " (&#39;10005&#39;, &#39;101&#39;, 85),\n",
       " (&#39;10005&#39;, &#39;103&#39;, 60)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[25]: [(&#39;10001&#39;, &#39;101&#39;, 98),\n (&#39;10001&#39;, &#39;102&#39;, 87),\n (&#39;10002&#39;, &#39;101&#39;, 75),\n (&#39;10002&#39;, &#39;102&#39;, 55),\n (&#39;10002&#39;, &#39;103&#39;, 80),\n (&#39;10003&#39;, &#39;102&#39;, 45),\n (&#39;10003&#39;, &#39;103&#39;, 75),\n (&#39;10004&#39;, &#39;101&#39;, 90),\n (&#39;10005&#39;, &#39;101&#39;, 85),\n (&#39;10005&#39;, &#39;103&#39;, 60)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c61df426-10d2-416d-af26-f404bd4928ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[24]: [(&#39;10001&#39;, (185, 2, &#39;AB&#39;)),\n",
       " (&#39;10004&#39;, (90, 1)),\n",
       " (&#39;10003&#39;, (120, 2, &#39;FC&#39;)),\n",
       " (&#39;10005&#39;, (145, 2, &#39;BF&#39;)),\n",
       " (&#39;10002&#39;, (210, 3, &#39;AC&#39;))]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[24]: [(&#39;10001&#39;, (185, 2, &#39;AB&#39;)),\n (&#39;10004&#39;, (90, 1)),\n (&#39;10003&#39;, (120, 2, &#39;FC&#39;)),\n (&#39;10005&#39;, (145, 2, &#39;BF&#39;)),\n (&#39;10002&#39;, (210, 3, &#39;AC&#39;))]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).reduceByKey(getCounts).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ed65938-2d27-41be-98e7-390c8b9d2c50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's look at some alternative implementations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f3fd5fc-2751-4d85-89c7-0a898d1f9e15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### foldByKey allows us to specify a zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea8f11a2-9dd1-4642-9e18-f51253c88c4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[26]: [(&#39;10004&#39;, (90, 1, &#39;FB&#39;)),\n",
       " (&#39;10001&#39;, (185, 2, &#39;AB&#39;)),\n",
       " (&#39;10002&#39;, (210, 3, &#39;AC&#39;)),\n",
       " (&#39;10003&#39;, (120, 2, &#39;FC&#39;)),\n",
       " (&#39;10005&#39;, (145, 2, &#39;BF&#39;))]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[26]: [(&#39;10004&#39;, (90, 1, &#39;FB&#39;)),\n (&#39;10001&#39;, (185, 2, &#39;AB&#39;)),\n (&#39;10002&#39;, (210, 3, &#39;AC&#39;)),\n (&#39;10003&#39;, (120, 2, &#39;FC&#39;)),\n (&#39;10005&#39;, (145, 2, &#39;BF&#39;))]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).foldByKey((0,0,\"\"), getCounts).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbdb969f-a43f-4b22-b96b-57633e1cab69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### <--- SOLUTION --->  \n",
    "__SOLUTION__   \n",
    "Student 10004 is assigned an initial 'F' grade. We have not met the foldByKey requirement because our zeroValue changes the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9027429c-32cf-4dc1-a5c5-63760dc83f03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Can we solve this problem using a combineByKey which provides more granular control over the parameters\n",
    "https://backtobazics.com/big-data/apache-spark-combinebykey-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "768c5263-39ed-4dc1-a231-d7b8a601bddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createCombiner(a):\n",
    "    return a\n",
    "\n",
    "def mergeValues(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]));\n",
    "\n",
    "def mergeCombiners(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]))\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .combineByKey(createCombiner,mergeValues,mergeCombiners)\\\n",
    "                       .mapValues(lambda x: ((x[0]/x[1]),x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68cd4cbe-04e4-4819-bbf2-b12898e73bad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gradesRDD.map(lambda x: (x[0], (x[2], 1))).combineByKey(createCombiner,mergeValues,mergeCombiners).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da68298d-dad3-4a9d-853e-610e14edeadc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### <--- SOLUTION --->  \n",
    "__SOLUTION__   \n",
    "We know that `reduceByKey` calls `combineByKey` under the hood. We have not changed anything here, so the result is exactly the same as in our first attempt when we used `reduceByKey`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94fabed6-34ac-419c-83c0-bb8880f4f490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### aggregateByKey requires a null and start value as well as two different functions. One to aggregate within partitions, and one to aggregate across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ad54865-0fa1-43ce-a465-80c5cf9f5d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def seqOp(a,b):\n",
    "    return(a[0] + b[0], a[1] + b[1], a[2]+toLetterGrade(b[2]))\n",
    "\n",
    "def combOp(a,b):\n",
    "    return (a+b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a73aeb1-b5ef-44e6-a562-73452d59e312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[32]: [(&#39;10001&#39;, &#39;101&#39;, 98),\n",
       " (&#39;10001&#39;, &#39;102&#39;, 87),\n",
       " (&#39;10002&#39;, &#39;101&#39;, 75),\n",
       " (&#39;10002&#39;, &#39;102&#39;, 55),\n",
       " (&#39;10002&#39;, &#39;103&#39;, 80),\n",
       " (&#39;10003&#39;, &#39;102&#39;, 45),\n",
       " (&#39;10003&#39;, &#39;103&#39;, 75),\n",
       " (&#39;10004&#39;, &#39;101&#39;, 90),\n",
       " (&#39;10005&#39;, &#39;101&#39;, 85),\n",
       " (&#39;10005&#39;, &#39;103&#39;, 60)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[32]: [(&#39;10001&#39;, &#39;101&#39;, 98),\n (&#39;10001&#39;, &#39;102&#39;, 87),\n (&#39;10002&#39;, &#39;101&#39;, 75),\n (&#39;10002&#39;, &#39;102&#39;, 55),\n (&#39;10002&#39;, &#39;103&#39;, 80),\n (&#39;10003&#39;, &#39;102&#39;, 45),\n (&#39;10003&#39;, &#39;103&#39;, 75),\n (&#39;10004&#39;, &#39;101&#39;, 90),\n (&#39;10005&#39;, &#39;101&#39;, 85),\n (&#39;10005&#39;, &#39;103&#39;, 60)]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bad44b4c-d0d7-49cd-af57-f83a08a011da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[36]: [(&#39;10001&#39;, (92.5, &#39;AB&#39;)),\n",
       " (&#39;10004&#39;, (90.0, &#39;B&#39;)),\n",
       " (&#39;10003&#39;, (60.0, &#39;FC&#39;)),\n",
       " (&#39;10005&#39;, (72.5, &#39;BF&#39;)),\n",
       " (&#39;10002&#39;, (70.0, &#39;CFC&#39;))]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[36]: [(&#39;10001&#39;, (92.5, &#39;AB&#39;)),\n (&#39;10004&#39;, (90.0, &#39;B&#39;)),\n (&#39;10003&#39;, (60.0, &#39;FC&#39;)),\n (&#39;10005&#39;, (72.5, &#39;BF&#39;)),\n (&#39;10002&#39;, (70.0, &#39;CFC&#39;))]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1, x[2])))\\\n",
    "         .aggregateByKey((0,0,\"\"),seqOp,combOp)\\\n",
    "         .mapValues(lambda x: ((x[0]/x[1]),x[2]))\\\n",
    "         .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f362292b-b8a4-40c4-9f2e-e2afb97c8a1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1, x[2]))).aggregateByKey((0,0,\"\"),seqOp,combOp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1e2a906-63be-43f2-8929-474b31000faf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "letterAccum = sc.accumulator(\"===\", StringAccumulatorParam())\n",
    "gradesRDD.foreach(lambda x: letterAccum.add(toLetterGrade(x[2])))\n",
    "print (letterAccum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d3847ebc-a0eb-4b64-8ed5-edf937908cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a684f3d-2a78-4686-8fc8-19d1564a02ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* join       \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.join     \n",
    "* leftOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.leftOuterJoin    \n",
    "* rightOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.rightOuterJoin    \n",
    "* fullOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.fullOuterJoin   \n",
    "* cartesian   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7e4560b-7615-4c85-91ad-c4d6e4459f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "sorted(x.fullOuterJoin(y).collect())\n",
    "#[('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff35bf46-084e-44e2-a254-fb890afec417",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted(x.rightOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "330dc169-7db5-4010-9461-f705e0016d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76a185e6-eb6c-44b2-9460-3d376eee5fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets load some data for the code examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d4c1642b-eca0-4de7-bfac-93ffa94c068b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d211122-c975-4209-b0e4-6f7a748e1241",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run as is\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ce66123-e871-4974-a42b-c81e6908be6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run as is\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e093c5d7-5167-4746-b195-e7dc2605d21f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run as is\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee65586f-f4bd-4063-b2ae-e9d5a3f24267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f3addfe-2589-4fc3-a099-c70cc0ee91a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person.join(graduateProgram, wrongJoinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fcf43546-f9e4-4db1-ba8f-c7ef135748a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark perfoms an \"inner\" join by default. But we can specify this explicitly.\n",
    "# Try different join types.\n",
    "joinType = \"outer\"\n",
    "#joinType = \"left_outer\"\n",
    "#joinType = \"right_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8878e69e-37bb-4530-b56a-58d5ec420478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41d01075-b615-4ea3-8469-9b6296a0e093",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Which keys do outer joins evaluate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08b38bd2-ebcc-4f39-b62c-45e38376d16d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A departure from traditional joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f69e93f-d540-40cd-afc0-c988013a2d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad9573c4-f85f-460c-814d-6f058d0fc23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gradProgram2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb471a11-b24a-4ed3-88f3-1c7553892bcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55692d1d-19c8-4b74-88c3-89c7fb30ec1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join\n",
    "joinType = \"left_semi\"\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60407001-3098-4b47-ad42-6a9e289728b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gradProgram2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8383b3ef-df95-4da6-b03c-1c9214fd29b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joinType = \"left_anti\"\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8675b59f-70f6-4df9-8ab1-201cb52a092d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Natural Joins\n",
    "\n",
    "__DANGER__: Natural joins make implicit guesses at the columns on which you would like to join. Why is this bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fe24a7f-1089-4b76-948c-4d826818f81a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cross (Cartesian) Joins\n",
    "Or, Cartesian products. Cross joinsare inner joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame with every single row in the right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3e043ab-7645-4642-892d-15d6313b9056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6bde892-c23b-4a4d-a707-5b897f8c9031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c89b2325-e4c6-4bb8-8b24-2ef0cdf26978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "90a9755d-374a-4b31-afe2-4faee40c4bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__DANGER__: How many rows would we end up with from a cross join if each table had 1000 rows?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "demo5_workbook",
   "notebookOrigID": 3829056957479987,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
