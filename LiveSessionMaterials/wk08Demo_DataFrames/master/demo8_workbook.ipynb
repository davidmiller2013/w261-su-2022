{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN THIS NOTEBOOK LOCALLY\n",
    "## This notebook is set up to run on Google dataproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk8 Demo - Advanced Spark - Pipelines and Optimizations with DataFrames\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "So far we've been using Spark's low level APIs. In particular, we've been using the RDD (Resilient Distiributed Datasets) API to implement Machine Learning algorithms from scratch. This week we're going to take a look at how Spark is used in a production setting. We'll look at DataFrames, SQL, and UDFs (User Defined Functions).  As discussed previously, we still need to understand the internals of Spark and MapReduce in general to write efficient and scalable code.\n",
    "\n",
    "In class today we'll get some practice working with larger data sets in Spark. We'll start with an introduction to efficiently storing data and approach a large dataset for analysis. After that we'll discuss a ranking problem which was covered in Chapter 6 of the High Performance Spark book and how we can apply that to our problem. We'll follow up with a discussion on things that could be done to make this more effiicent.\n",
    "* ... __describe__ differences between data serialization formats.\n",
    "* ... __choose__ a data serialization format based on use case.\n",
    "* ... __change__ submission arguements for a `SparkSession`.\n",
    "* ... __set__ custom configuration for a `SparkSession`.\n",
    "* ... __describe__ and __create__ a data pipeline for analysis.\n",
    "* ... __use__ a user defined function (UDF).\n",
    "* ... __understand__ feature engineering and aggregations in Spark.\n",
    "\n",
    "__`Additional Resources:`__ Writing performant code in Spark requires a lot of thought. Holden's High Performance Spark book covers this topic very well. In addition, Spark - The Definitive Guide, by Bill Chambers and Matei Zaharia, provides some recent developments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is running on dataproc with the following setup\n",
    "\n",
    "```{bash}\n",
    "BUCKET=\"w261-bucket\"\n",
    "CLUSTER=\"w261-demo\"\n",
    "PROJECT=\"w261-216504\"\n",
    "JUPYTER_PORT=\"8123\"\n",
    "PORT=\"10000\"\n",
    "ZONE=$(gcloud config get-value compute/zone)\n",
    "\n",
    "# CREATE DATAPROC CLUSTER\n",
    "gcloud dataproc clusters create ${CLUSTER} \\\n",
    "    --metadata \"JUPYTER_PORT=${JUPYTER_PORT}\" \\\n",
    "    --metadata \"JUPYTER_CONDA_PACKAGES=numpy:pandas:scipy:pyarrow\" \\\n",
    "    --metadata \"JUPYTER_CONDA_CHANNELS=conda-forge\" \\\n",
    "    --project ${PROJECT} \\\n",
    "    --bucket ${BUCKET} \\\n",
    "    --image-version \"1.3.10-deb9\" \\\n",
    "    --initialization-actions \\\n",
    "       gs://dataproc-initialization-actions/jupyter/jupyter.sh \\\n",
    "    --num-preemptible-workers=4 \\\n",
    "    --num-workers=2 \\\n",
    "    --worker-machine-type=n1-standard-8 \\\n",
    "    --master-machine-type=n1-standard-8\n",
    "    \n",
    "# CREATE SOCKS PROXY\n",
    "gcloud compute ssh ${CLUSTER}-m \\\n",
    "    --project=${PROJECT} \\\n",
    "    --zone=${ZONE}  \\\n",
    "    --ssh-flag=\"-D\" \\\n",
    "    --ssh-flag=${PORT} \\\n",
    "    --ssh-flag=\"-N\"\n",
    "\n",
    "```\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# USE SOCKS PROXY (if your local computer is linux)\n",
    "/usr/bin/google-chrome \\\n",
    "  --proxy-server=\"socks5://localhost:${PORT}\" \\\n",
    "  --user-data-dir=/tmp/${CLUSTER}-m   \n",
    "\n",
    "# USE SOCKS PROXY (if your local computer is a mac)\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
    "  --proxy-server=\"socks5://localhost:${PORT}\" \\\n",
    "  --user-data-dir=/tmp/${CLUSTER}-m  \n",
    "\n",
    "# MORE INFO ON SETTING UP SOCKS PROXIES IN GCP:\n",
    "# https://cloud.google.com/solutions/connecting-securely#socks-proxy-over-ssh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Today we'll be using GSOD weather station data, avaliable from Google in BigQuery.\n",
    "\n",
    "Since this is a decent sized dataset (21 GB uncompressed) we won't be running code, but rather reviewing the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r48f1a0402fac4c3c_000001694ef31157_1 ... (50s) Current status: DONE   \n",
      "Waiting on bqjob_r68977011483455a1_000001694ef3e414_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "# Get data from BigQuery into Google Cloud Storage as GZIP compressed CSV files\n",
    "!bq --location=US extract --compression GZIP 'bigquery-public-data:samples.gsod' gs://w261-bucket/gsod/gsod-*.csv.gz\n",
    "!bq --location=US extract --compression GZIP 'bigquery-public-data:noaa_gsod.stations' gs://w261-bucket/gsod/stations.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w261-demo-m.c.w261-216504.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5911657d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we show how to do a custom configuration\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'w261-demo-m'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,/root/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.port', '43385'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.maxResultSize', '3840m'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs://w261-demo-m/user/spark/eventlog'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip<CPS>{{PWD}}/com.databricks_spark-avro_2.11-4.0.0.jar<CPS>{{PWD}}/org.slf4j_slf4j-api-1.7.5.jar<CPS>{{PWD}}/org.apache.avro_avro-1.7.6.jar<CPS>{{PWD}}/org.codehaus.jackson_jackson-core-asl-1.9.13.jar<CPS>{{PWD}}/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar<CPS>{{PWD}}/com.thoughtworks.paranamer_paranamer-2.3.jar<CPS>{{PWD}}/org.xerial.snappy_snappy-java-1.0.5.jar<CPS>{{PWD}}/org.apache.commons_commons-compress-1.4.1.jar<CPS>{{PWD}}/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.eventLog.dir', 'hdfs://w261-demo-m/user/spark/eventlog'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1551805896912_0003'),\n",
       " ('spark.yarn.historyServer.address', 'w261-demo-m:18080'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1551805896912_0003'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.driver.host', 'w261-demo-m.c.w261-216504.internal'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.jars.packages', 'com.databricks:spark-avro_2.11:4.0.0'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://w261-demo-m.c.w261-216504.internal:4040'),\n",
       " ('spark.executor.memory', '11171m'),\n",
       " ('spark.yarn.secondary.jars',\n",
       "  'com.databricks_spark-avro_2.11-4.0.0.jar,org.slf4j_slf4j-api-1.7.5.jar,org.apache.avro_avro-1.7.6.jar,org.codehaus.jackson_jackson-core-asl-1.9.13.jar,org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,com.thoughtworks.paranamer_paranamer-2.3.jar,org.xerial.snappy_snappy-java-1.0.5.jar,org.apache.commons_commons-compress-1.4.1.jar,org.tukaani_xz-1.0.jar'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.executorEnv.PYTHONHASHSEED', '0'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://w261-demo-m:8088/proxy/application_1551805896912_0003'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.sql.parquet.cacheMetadata', 'false'),\n",
       " ('spark.driver.memory', '7680m'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.cbo.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Structured API - Datasets, DataFrames, and SQL Tables and Views\n",
    "\n",
    "A Dataset is a distributed collection of data. Datasets provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n",
    "    \n",
    "This makes the analysis of data similar to how we would do analysis with Python's Pandas or R's dataframes. Spark DataFrames are heavily inspired by Pandas and we're actually able to create Pandas user-defined functions (UDFs) to use with Spark which leverage the Apache Arrow project to vectorized computation instead of row-by-row operations. This can lead to significant performance boosts for large datasets. \n",
    "    \n",
    "SQL Tables and Views are basically the same thing as DataFrames. We simply just execute SQL against them instead of DataFrame code  *(Defintive Guide, pg. 50)*. You can choose to express some of your data manipulations in SQL and others in DataFrames and they will compile to the same underlying code. *(Defintive Guide, pg. 179)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __DISCUSSION QUESTIONS:__ \n",
    " * _Why would we want to use RDDs in this class over DataFrames?_\n",
    " * _What is a UDF? Why do we need to create them?_\n",
    " * _What is vectorized computation and how does that differ from row-by-row function calls_\n",
    " * _How is a Dataset different than a DataFrame?_\n",
    " * _Are Datasets avaliable in the Python API?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INSTRUCTOR TALKING POINTS__\n",
    "* Why would we want to use RDDs in this class over DataFrames?\n",
    "> custom partitioning; some things can only be done in the RDD API\n",
    "* What is a UDF? Why do we need to create them?\n",
    "> User Defined Function; to extend the Spark functionality; to perform row-by-row operations\n",
    "* What is vectorized computation and how does that differ from row-by-row function calls\n",
    "> computations are executed concurrently. This is very efficient. For example, use numpy when you can. numpy is implemented in C, and C implements vectorized operations\n",
    "* How is a Dataset different than a DataFrame?\n",
    "> Datasets are strongly typed. Datasets have a slightly richer API. Can be more optimized due to type casting.\n",
    "* Are Datasets avaliable in the Python API?\n",
    "> No. Python is a dynamically typed language and as such is not comaptible with datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Data Serialization Formats. \n",
    "This week you read [Format Wars](http://www.svds.com/dataformats/) which covered the characteristics, structure, and differences between raw text, sequence, Avro, Parquet, and ORC data serializations. \n",
    "\n",
    "There were several points discussed: \n",
    "\n",
    "* Human Readable\n",
    "* Row vs Column Oriented\n",
    "* Read vs Write performance\n",
    "* Appendable\n",
    "* Splittable\n",
    "* Metadata storage\n",
    "\n",
    "*For additional information see Definitive Guide, Chapter 9: Data Sources, pg.153*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's understand our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/gsod-000000000000.csv.gz gs://w261-bucket/gsod/gsod-000000000000.csv.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have several compressed CSV files as we expect based on our bq command specifying compressions. BigQuery was nice enough to split the files into 30 MB chunks so that our analysis will be partitioned nicely for ingestion.\n",
    "\n",
    "Now let's try to ingest these CSV's without any special commands or unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = spark.read.option(\"header\", \"true\").csv(\"gs://w261-bucket/gsod/gsod-*.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(station_number='39730', wban_number='99999', year='1929', month='11', day='25', mean_temp='49', num_mean_temp_samples='4', mean_dew_point='40.200000762939453', num_mean_dew_point_samples='4', mean_sealevel_pressure=None, num_mean_sealevel_pressure_samples=None, mean_station_pressure=None, num_mean_station_pressure_samples=None, mean_visibility='10.899999618530273', num_mean_visibility_samples='4', mean_wind_speed='19.200000762939453', num_mean_wind_speed_samples='4', max_sustained_wind_speed='36.900001525878906', max_gust_wind_speed=None, max_temperature='46.900001525878906', max_temperature_explicit='true', min_temperature=None, min_temperature_explicit=None, total_precipitation='0', snow_depth=None, fog='false', rain='false', snow='false', hail='false', thunder='false', tornado='false')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark is smart enough to unzip the .gz files\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_number: string (nullable = true)\n",
      " |-- wban_number: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- mean_temp: string (nullable = true)\n",
      " |-- num_mean_temp_samples: string (nullable = true)\n",
      " |-- mean_dew_point: string (nullable = true)\n",
      " |-- num_mean_dew_point_samples: string (nullable = true)\n",
      " |-- mean_sealevel_pressure: string (nullable = true)\n",
      " |-- num_mean_sealevel_pressure_samples: string (nullable = true)\n",
      " |-- mean_station_pressure: string (nullable = true)\n",
      " |-- num_mean_station_pressure_samples: string (nullable = true)\n",
      " |-- mean_visibility: string (nullable = true)\n",
      " |-- num_mean_visibility_samples: string (nullable = true)\n",
      " |-- mean_wind_speed: string (nullable = true)\n",
      " |-- num_mean_wind_speed_samples: string (nullable = true)\n",
      " |-- max_sustained_wind_speed: string (nullable = true)\n",
      " |-- max_gust_wind_speed: string (nullable = true)\n",
      " |-- max_temperature: string (nullable = true)\n",
      " |-- max_temperature_explicit: string (nullable = true)\n",
      " |-- min_temperature: string (nullable = true)\n",
      " |-- min_temperature_explicit: string (nullable = true)\n",
      " |-- total_precipitation: string (nullable = true)\n",
      " |-- snow_depth: string (nullable = true)\n",
      " |-- fog: string (nullable = true)\n",
      " |-- rain: string (nullable = true)\n",
      " |-- snow: string (nullable = true)\n",
      " |-- hail: string (nullable = true)\n",
      " |-- thunder: string (nullable = true)\n",
      " |-- tornado: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that all fileds are strings. CSV does not have schema!\n",
    "data_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114420316, 31)\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print((data_csv.count(), len(data_csv.columns)))\n",
    "# user: the time it takes to execute user written code\n",
    "# sys: the time to execute kernel instructions\n",
    "# total: sum of user and sys\n",
    "# wall: includes I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that's nice we didn't even have to handle the decompression and it saves a ton on disk space! Next we're going to save this in a few different serializations so that we can see the effect on disk space.\n",
    "\n",
    "Also notice that since we have 114 million observations and 31 columns we should see some huge performance boosts for compression in general and particularly columnar compression with parquet since it takes into account the data type to improve compression further. While row based compression will be less.\n",
    "\n",
    "_Which Data Serialization do you think will do best?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do these look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 data types below\n",
    "\n",
    "- Compressed CSV\n",
    "- Parquet\n",
    "- Avro\n",
    "- CSV\n",
    "\n",
    "Of these 3 are row oriented and 1 is column oriented. We have over 100M rows and 31 columns. Columnar compression should do fairly well in this scenerio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.96 GiB    gs://w261-bucket/gsod/gsod*.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Our original Compressed data already exists\n",
    "!gsutil du -sh gs://w261-bucket/gsod/gsod*.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.61 GiB    gs://w261-bucket/gsod/data.parquet\n"
     ]
    }
   ],
   "source": [
    "data_csv.write.format(\"parquet\").save(\"gs://w261-bucket/gsod/data.parquet\")\n",
    "!gsutil du -sh gs://w261-bucket/gsod/data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.37 GiB    gs://w261-bucket/gsod/data.avro\n"
     ]
    }
   ],
   "source": [
    "data_csv.write.format(\"com.databricks.spark.avro\").save(\"gs://w261-bucket/gsod/data.avro\")\n",
    "!gsutil du -sh gs://w261-bucket/gsod/data.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.74 GiB   gs://w261-bucket/gsod/data.csv\n"
     ]
    }
   ],
   "source": [
    "data_csv.write.format(\"com.databricks.spark.csv\").save('gs://w261-bucket/gsod/data.csv')\n",
    "!gsutil du -sh gs://w261-bucket/gsod/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do these compare for simple computations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to read in the data again to ensure we're working with non-cached versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = spark.read.parquet(\"gs://w261-bucket/gsod/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count: \n",
    "Parquet keeps metadata about the data in order to compute some calculations extremely quickly such as row counts. Notice the difference in wall time vs CPU times. Reading in parquet (I/O) is much more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114420316"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114420316"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of a column: \n",
    "Parquet is column oriented so it can go through the sequence of data in one step instead of taking each row. This should have much higher performance. On the other hand, there is some overhead in writing the data to parquet in the first place. As always, there are tradeoffs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 3.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(avg(max_temperature)=43.51674998177965)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_parquet.agg(F.avg(data_parquet.max_temperature)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 22.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(avg(max_temperature)=43.51674998177989)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_csv.agg(F.avg(data.max_temperature)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __DISCUSSION QUESTIONS:__ For each key term from the reading, briefly explain what it means in the context of this demo code. Specifically:\n",
    " * _What is the compression ratio for the parquet to csv file?_\n",
    " * _Which serialization would query a column faster?_\n",
    " * _Which types (data types) of columns do you think has the best compression for parquet?_\n",
    " * _When should you use flat files vs other data formats?_\n",
    " * _If we want to do analysis with lots of aggregations what serialization should we use?_\n",
    " * _Is there any downside to Parquet?_\n",
    " * _If you had to partition data into days as new data comes in with aggregations happening at end of day how would you operationalize this? For example, we collect weather data throughout the day, and at the end of the day we'd like to do some analysis_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INSTRUCTOR TALKING POINTS__\n",
    "* What is the compression ratio for the parquet to csv file?\n",
    "> We have 1.7G/21G = 0.081 or 8.1% of original size\n",
    "\n",
    "* _Which serialization would query a column faster?_\n",
    "> Parquet has a columnar format therefore a column of data has faster access and only needs to grab a subset of data\n",
    "\n",
    "* _Which types of columns do you think has the best compression for parquet?_\n",
    "> Columns with repeated content will have better compressions such as categorical columns will have very high compression ratios, especially if they're integers since parquet has enhanced compression for types with smaller storage requirements. How many bytes does it take to store a String, a Float, an Int?\n",
    "\n",
    "* _When should you use flat files vs other data formats?_\n",
    "> If you need human readable data or you have small data sets. Interoperability - for sharing with other teams. Don't send Bob in accounting a parquet file! Bob will try to open it in excel and he'll get an error, call IT, and IT will tell Bob to clear his cookies and restart his computer. Bob will not be impressed.\n",
    "\n",
    "* _If we want to do analysis with lots of aggregations what serialization should we use?_\n",
    "> Parquet\n",
    "\n",
    "* _Is there any downside to Parquet?_\n",
    "> Parquet is non-appendable (immutable) which means that if we have new data coming in we can't grow the dataset with parquet. Parquet datasets are typically used for batch analysis after the data has reached a final state, such as on a date roll-over.\n",
    " \n",
    "* _If you had to partition data into days as new data comes in with aggregations happening at end of day how would you operationalize this?_\n",
    "> Data coming in for a day is streamed into an Avro file which handles appends seamlessly, then once the day has completed and a new partition for data is created a batch job can convert the avro file into a parquet file for the DS/Analyst team to query against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Working with DataFrames and simple User-Defined Functions (UDFs)\n",
    "\n",
    "In this example we're going to do some simple analysis of our data using built in spark functions. We'll look into UDFs and use a few instances of them to process our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in Spark functions are always more efficient\n",
    "from pyspark.sql import types\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "timed = data_parquet.withColumn(\"time\", \n",
    "                                F.concat(F.col(\"year\"), \n",
    "                                F.lit(\"-\"), F.col(\"month\"), \n",
    "                                F.lit(\"-\"), F.col(\"day\")) \\\n",
    "                                .cast(types.TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               time|\n",
      "+-------------------+\n",
      "|1929-11-25 00:00:00|\n",
      "|1931-07-27 00:00:00|\n",
      "|1931-03-30 00:00:00|\n",
      "|1931-12-20 00:00:00|\n",
      "|1931-10-17 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 949 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "timed.select('time').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 771 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(time=datetime.datetime(1929, 11, 25, 0, 0))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "timed.select('time').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple UDF for converting year, month, day to timestamps\n",
    "def create_date_from_parts(year, month, day):\n",
    "    return f'{year}-{month}-{day}'\n",
    "\n",
    "create_date_udf = F.udf(create_date_from_parts, types.StringType())\n",
    "timed_udf = data_parquet.withColumn(\"date\", create_date_udf('year', 'month', 'day').cast(types.TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not cast the string to a timestamp inside the UDF? \n",
    "# We don't want to perform the cast row by row. It's always better to use built in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 2.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(station_number='39730', wban_number='99999', year='1929', month='11', day='25', mean_temp='49', num_mean_temp_samples='4', mean_dew_point='40.200000762939453', num_mean_dew_point_samples='4', mean_sealevel_pressure=None, num_mean_sealevel_pressure_samples=None, mean_station_pressure=None, num_mean_station_pressure_samples=None, mean_visibility='10.899999618530273', num_mean_visibility_samples='4', mean_wind_speed='19.200000762939453', num_mean_wind_speed_samples='4', max_sustained_wind_speed='36.900001525878906', max_gust_wind_speed=None, max_temperature='46.900001525878906', max_temperature_explicit='true', min_temperature=None, min_temperature_explicit=None, total_precipitation='0', snow_depth=None, fog='false', rain='false', snow='false', hail='false', thunder='false', tornado='false', date=datetime.datetime(1929, 11, 25, 0, 0))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "timed_udf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's many things we could do from here but there are some important performance considerations when using UDFs. \n",
    "\n",
    "\n",
    "> User-defined functions and user-defined aggregate functions provide you with ways to extend the DataFrame and SQL APIs with your own custom code while keeping the Catalyst optimizer. The Dataset API (see “Datasets” on page 62) is another performant option for much of what you can do with UDFs and UDAFs. This is quite useful for performance, since otherwise you would need to convert the data to an RDD (and potentially back again) to perform arbitrary functions, which is quite expensive. (HP Spark pg 66) \n",
    "\n",
    "\n",
    "UDFs are typically much slower than built-in Spark functionality. The reason for this is becauase they have to serialize and deserialize the data for every row that the function is applied to. There have been recent improvements to UDF for some analytical results with Pandas UDFs that return scalars or groupby maps. Some more information about why UDFs are inefficent can be found here https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/\n",
    "\n",
    "Pandas UDFs solve the serialization issue by vectorizing the inputs and outputs, decreasing the serialziation from 3-100x; however, it isn't a golden bullet. See this blog for details http://garrens.com/blog/2018/03/04/using-new-pyspark-2-3-vectorized-pandas-udfs-lessons/\n",
    "\n",
    "See also: http://sparklingpandas.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__DISCUSSION QUESTION:__ \n",
    "* What is the task here? What did we really accomplish?\n",
    "* What type does the UDF create_date_from_parts return?\n",
    "* What information is being stored in the data frame? Is  there anything inefficient about this data structure? \n",
    "* What types of situations would lead to an inefficeint data structure in DataFrames? Could we be more efficient using an RDD in those situations?\n",
    "* What questions would you ask of this table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INSTRUCTOR TALKING POINTS__\n",
    "* What is the task here? What did we really accomplish?\n",
    "> We read in data and created a datetime type so we have a single column we can organize data by instead of year, month, day.\n",
    "\n",
    "* What type does the UDF create_date_from_parts return?\n",
    "> String\n",
    "\n",
    "* What information is being stored in the data frame? Is there anything inefficient about this data structure? \n",
    "> The data is mostly numerical. The data is stored effiicently since the DataFrame is mostly populated.\n",
    "\n",
    "* What types of situations would lead to an inefficeint data structure in DataFrames? Could we be more efficient using an RDD in those situations?\n",
    "> Sparse DataFrames have a more efficient data representation in RDDs. An example would be a multi-dimensional cube or pivot table (2d).\n",
    "\n",
    "* What questions would you ask of this table?\n",
    "> What is the change in temperature over time as a function of lat/long. In other words, Do we have global warming? Let's look at it by looking at average tempertues by latitude have evolved over time.\n",
    "\n",
    "UDFs, UDAFs, and Datasets all provide ways to intermix arbitrary code with Spark SQL.\n",
    "\n",
    "We recommend you write your UDFs in Java or Scala-the small amount of time it atkes to write the function in Java or Scala will always yield significant speed ups. And you can still use the function from within python! (p.113 - Spark, The Definitive Guide)   \n",
    "Scala UDF in python example:   \n",
    "https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9   \n",
    "https://github.com/johnmuller87/spark-udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 4. EDA\n",
    "\n",
    "In this exercise we'll do some basic EDA/Sanity checks of our DataFrame, and start preparing it for our analysis in exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_number: string (nullable = true)\n",
      " |-- wban_number: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- mean_temp: string (nullable = true)\n",
      " |-- num_mean_temp_samples: string (nullable = true)\n",
      " |-- mean_dew_point: string (nullable = true)\n",
      " |-- num_mean_dew_point_samples: string (nullable = true)\n",
      " |-- mean_sealevel_pressure: string (nullable = true)\n",
      " |-- num_mean_sealevel_pressure_samples: string (nullable = true)\n",
      " |-- mean_station_pressure: string (nullable = true)\n",
      " |-- num_mean_station_pressure_samples: string (nullable = true)\n",
      " |-- mean_visibility: string (nullable = true)\n",
      " |-- num_mean_visibility_samples: string (nullable = true)\n",
      " |-- mean_wind_speed: string (nullable = true)\n",
      " |-- num_mean_wind_speed_samples: string (nullable = true)\n",
      " |-- max_sustained_wind_speed: string (nullable = true)\n",
      " |-- max_gust_wind_speed: string (nullable = true)\n",
      " |-- max_temperature: string (nullable = true)\n",
      " |-- max_temperature_explicit: string (nullable = true)\n",
      " |-- min_temperature: string (nullable = true)\n",
      " |-- min_temperature_explicit: string (nullable = true)\n",
      " |-- total_precipitation: string (nullable = true)\n",
      " |-- snow_depth: string (nullable = true)\n",
      " |-- fog: string (nullable = true)\n",
      " |-- rain: string (nullable = true)\n",
      " |-- snow: string (nullable = true)\n",
      " |-- hail: string (nullable = true)\n",
      " |-- thunder: string (nullable = true)\n",
      " |-- tornado: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read.option(\"header\", \"true\").csv(\"gs://w261-bucket/gsod/stations.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+-------+-----+----+-----+------+-------+--------+--------+\n",
      "|  usaf| wban|                name|country|state|call|  lat|   lon|   elev|   begin|     end|\n",
      "+------+-----+--------------------+-------+-----+----+-----+------+-------+--------+--------+\n",
      "|007018|99999|WXPOD 7018       ...|   null| null|null|    0|     0|+7018.0|20110309|20130730|\n",
      "|007026|99999|WXPOD 7026       ...|     AF| null|null|    0|     0|+7026.0|20120713|20170822|\n",
      "|007070|99999|WXPOD 7070       ...|     AF| null|null|    0|     0|+7070.0|20140923|20150926|\n",
      "|008268|99999|WXPOD8278        ...|     AF| null|null|32.95|65.567|+1156.7|20100519|20120323|\n",
      "|008307|99999|WXPOD 8318       ...|     AF| null|null|    0|     0|+8318.0|20100421|20100421|\n",
      "+------+-----+--------------------+-------+-----+----+-----+------+-------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter for just the US since this is a US based dataset\n",
    "stations_us = stations.filter(F.col('Country')=='US')\n",
    "\n",
    "'''\n",
    "# Equivalently, we could write:\n",
    "stations_us = stations.where('Country'=='US')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods to perform this operation: you can use `where` or `filter` (as above) and they will both perform the same operation. (pg 74 - Spark, The Definitive Guide). Remember that the DataFrame API and Spark SQL compile to the same execution plan.\n",
    "\n",
    "Take a look at the explain plan for pushdown predicates: (`PushedFilters: [IsNotNull(station_number)]`). (More info on pg 169, 325 - Spark, The Definitive Guide). Spark pushes down predicates to the data layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to bring that back to our timed dataframe\n",
    "timed_stations = timed.join(F.broadcast(stations_us), stations_us.usaf==timed.station_number, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [station_number#424], [usaf#721], Inner, BuildRight\n",
      ":- *(2) Project [station_number#424, wban_number#425, year#426, month#427, day#428, mean_temp#429, num_mean_temp_samples#430, mean_dew_point#431, num_mean_dew_point_samples#432, mean_sealevel_pressure#433, num_mean_sealevel_pressure_samples#434, mean_station_pressure#435, num_mean_station_pressure_samples#436, mean_visibility#437, num_mean_visibility_samples#438, mean_wind_speed#439, num_mean_wind_speed_samples#440, max_sustained_wind_speed#441, max_gust_wind_speed#442, max_temperature#443, max_temperature_explicit#444, min_temperature#445, min_temperature_explicit#446, total_precipitation#447, ... 8 more fields]\n",
      ":  +- *(2) Filter isnotnull(station_number#424)\n",
      ":     +- *(2) FileScan parquet [station_number#424,wban_number#425,year#426,month#427,day#428,mean_temp#429,num_mean_temp_samples#430,mean_dew_point#431,num_mean_dew_point_samples#432,mean_sealevel_pressure#433,num_mean_sealevel_pressure_samples#434,mean_station_pressure#435,num_mean_station_pressure_samples#436,mean_visibility#437,num_mean_visibility_samples#438,mean_wind_speed#439,num_mean_wind_speed_samples#440,max_sustained_wind_speed#441,max_gust_wind_speed#442,max_temperature#443,max_temperature_explicit#444,min_temperature#445,min_temperature_explicit#446,total_precipitation#447,... 7 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://w261-bucket/gsod/data.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(station_number)], ReadSchema: struct<station_number:string,wban_number:string,year:string,month:string,day:string,mean_temp:str...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [usaf#721, wban#722, name#723, country#724, state#725, call#726, lat#727, lon#728, elev#729, begin#730, end#731]\n",
      "      +- *(1) Filter ((isnotnull(Country#724) && (Country#724 = US)) && isnotnull(usaf#721))\n",
      "         +- *(1) FileScan csv [usaf#721,wban#722,name#723,country#724,state#725,call#726,lat#727,lon#728,elev#729,begin#730,end#731] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://w261-bucket/gsod/stations.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,US), IsNotNull(usaf)], ReadSchema: struct<usaf:string,wban:string,name:string,country:string,state:string,call:string,lat:string,lon...\n"
     ]
    }
   ],
   "source": [
    "timed_stations.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will automatically broadcast a small table, it's usually best to let Spark decide. See explain plan below. Notice that Spark broadcast the join for us even though we took that function out in our code. (p.151 Defintive Guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed_stations_NB = timed.join(stations_us, stations_us.usaf==timed.station_number, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [station_number#165], [usaf#270], Inner, BuildRight\n",
      ":- *(2) Project [station_number#165, wban_number#166, year#167, month#168, day#169, mean_temp#170, num_mean_temp_samples#171, mean_dew_point#172, num_mean_dew_point_samples#173, mean_sealevel_pressure#174, num_mean_sealevel_pressure_samples#175, mean_station_pressure#176, num_mean_station_pressure_samples#177, mean_visibility#178, num_mean_visibility_samples#179, mean_wind_speed#180, num_mean_wind_speed_samples#181, max_sustained_wind_speed#182, max_gust_wind_speed#183, max_temperature#184, max_temperature_explicit#185, min_temperature#186, min_temperature_explicit#187, total_precipitation#188, ... 8 more fields]\n",
      ":  +- *(2) Filter isnotnull(station_number#165)\n",
      ":     +- *(2) FileScan parquet [station_number#165,wban_number#166,year#167,month#168,day#169,mean_temp#170,num_mean_temp_samples#171,mean_dew_point#172,num_mean_dew_point_samples#173,mean_sealevel_pressure#174,num_mean_sealevel_pressure_samples#175,mean_station_pressure#176,num_mean_station_pressure_samples#177,mean_visibility#178,num_mean_visibility_samples#179,mean_wind_speed#180,num_mean_wind_speed_samples#181,max_sustained_wind_speed#182,max_gust_wind_speed#183,max_temperature#184,max_temperature_explicit#185,min_temperature#186,min_temperature_explicit#187,total_precipitation#188,... 7 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://w261-bucket/gsod/data.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(station_number)], ReadSchema: struct<station_number:string,wban_number:string,year:string,month:string,day:string,mean_temp:str...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [usaf#270, wban#271, name#272, country#273, state#274, call#275, lat#276, lon#277, elev#278, begin#279, end#280]\n",
      "      +- *(1) Filter ((isnotnull(Country#273) && (Country#273 = US)) && isnotnull(usaf#270))\n",
      "         +- *(1) FileScan csv [usaf#270,wban#271,name#272,country#273,state#274,call#275,lat#276,lon#277,elev#278,begin#279,end#280] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://w261-bucket/gsod/stations.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,US), IsNotNull(usaf)], ReadSchema: struct<usaf:string,wban:string,name:string,country:string,state:string,call:string,lat:string,lon...\n"
     ]
    }
   ],
   "source": [
    "timed_stations_NB.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's only keep what we care about so we minimize our pain\n",
    "# The * syntaxt allows us to paa an arbitrary number of parameters\n",
    "keep_columns = ['station_number', 'mean_temp', 'time', 'lat', 'lon']\n",
    "temp = timed_stations.select(*keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recast types\n",
    "temp = temp.withColumn(\"mean_temp\", temp[\"mean_temp\"].cast(types.DoubleType()))\n",
    "temp = temp.withColumn(\"lat\", temp[\"lat\"].cast(types.DoubleType()))\n",
    "temp = temp.withColumn(\"lon\", temp[\"lon\"].cast(types.DoubleType()))\n",
    "temp = temp.withColumn(\"station_number\", temp[\"station_number\"].cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-------------------+\n",
      "|summary|   station_number|        mean_temp|               lat|                lon|\n",
      "+-------+-----------------+-----------------+------------------+-------------------+\n",
      "|  count|       4573238050|       4573238050|        4569318291|         4569318291|\n",
      "|   mean|998212.5947369742|53.70208795128997|39.878959617922426|-100.41609814187261|\n",
      "| stddev|22106.62444452674|21.44938403487329| 8.916933006518253| 27.294152331937006|\n",
      "|    min|           690014|            -69.0|           -60.483|            -179.63|\n",
      "|    max|           999999|            110.0|             80.13|            179.583|\n",
      "+-------+-----------------+-----------------+------------------+-------------------+\n",
      "\n",
      "CPU times: user 20 ms, sys: 32 ms, total: 52 ms\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# How is our dataframe looking? We did filter a bunch of data\n",
    "temp.describe().show()\n",
    "# You could also use 'summary()' to extract individual numbers for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of the data in histograms\n",
    "# def plot_hist(hist_list):\n",
    "#     pd.DataFrame(\n",
    "#         list(zip(*hist_list)), \n",
    "#         columns=['bin', 'frequency']\n",
    "#     ).set_index(\n",
    "#         'bin'\n",
    "#     ).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(labels,values):\n",
    "    df = pd.DataFrame({'lab':labels, 'val':values})\n",
    "    df.plot.bar(x='lab', y='val', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[station_number: int, mean_temp: double, time: timestamp, lat: double, lon: double]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "temp.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use filter instead of rdd, and take advantage of predicate pushdown\n",
    "import math\n",
    "def makeHistogram(_min,_max,numBuckets,colName):\n",
    "    _range = list(range(math.floor(_min), math.ceil(_max), round((abs(_min)+abs(_max))/numBuckets)))\n",
    "    _counts = np.zeros(len(_range))\n",
    "    for idx, val in enumerate(_range):\n",
    "        if idx < len(_range)-1:\n",
    "            _counts[idx] = temp.filter(F.col(colName) >= _range[idx]) \\\n",
    "                               .filter(F.col(colName) <= _range[idx+1]) \\\n",
    "                               .count()\n",
    "    plot_hist(_range,_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92 ms, sys: 12 ms, total: 104 ms\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "makeHistogram(-69.0,110.0,11,'mean_temp')\n",
    "# Looking at the values on the x-axis, it makes sense that US temperatures would fall into this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 12 ms, total: 88 ms\n",
      "Wall time: 57.8 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "makeHistogram(-179.63,179.583,11,'lon')\n",
    "# Again, looking at the values on the x-axis, it makes sense that US longitudes would fall into this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 ms, sys: 20 ms, total: 88 ms\n",
      "Wall time: 54.8 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "makeHistogram(-60.483,80.13,11,'lat')\n",
    "# Again, looking at the values on the x-axis, it makes sense that US latitudes would fall into this range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the RDD method (below) took almost an hour to run for a single plot. Even after caching the temp DF. Bummer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# temp_hist = temp.select('mean_temp').rdd.flatMap(lambda x: x).histogram(11)\n",
    "# plot_hist(temp_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../screenshots/lost-proxy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# temp_hist = temp.select('lon').rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "# # Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "# plot_hist(temp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# temp_hist = temp.select('lat').rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "# # Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "# plot_hist(temp_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Analysis\n",
    "\n",
    "In this section we're going to perform some aggregations to answer the question of \"Is there global warming?\" Don't take it too seriously, it's just an exercise! We're going to prepare our data so that we can draw an interactive graph displaying the change in avarage temperatures over time, as a function of latitude.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"temp-table-01.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a struct to build a composite key. The struct allows us to preserve the individual columns \n",
    "# while giving us the ability to use the composite to do a groupBy\n",
    "temp = temp.withColumn('time-lat', F.struct('time','lat'))\n",
    "daily_average_at_latitude = temp.select('time-lat','mean_temp').groupBy(\"time-lat\").agg(F.avg('mean_temp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re used to RDDs you might be concerned by groupBy, but it is now a safe operation on thanks to the Spark SQL DataFrames optimizer, which automatically pipelines our reductions, avoiding giant shuffles and mega records. (HP Spark, pg. 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time-lat: struct (nullable = false)\n",
      " |    |-- time: timestamp (nullable = true)\n",
      " |    |-- lat: double (nullable = true)\n",
      " |-- avg(mean_temp): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_average_at_latitude.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"time-lat-avg2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is here to make it easier to reason with the column names, flattens structs\n",
    "def flatten_df(nested_df):\n",
    "    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n",
    "\n",
    "    flat_df = nested_df.select(flat_cols +\n",
    "                               [F.col(nc+'.'+c).alias(nc+'_'+c)\n",
    "                                for nc in nested_cols\n",
    "                                for c in nested_df.select(nc+'.*').columns])\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_average_at_latitude = flatten_df(daily_average_at_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- avg(mean_temp): double (nullable = true)\n",
      " |-- time-lat_time: timestamp (nullable = true)\n",
      " |-- time-lat_lat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_average_at_latitude.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the average on each latitude. \n",
    "# Latitudes are very granular, so it makes sense to round them and group by these coarser values\n",
    "daily_average_at_latitude = daily_average_at_latitude.withColumn('time-rounded-lat', F.struct('time-lat_time',F.round(daily_average_at_latitude['time-lat_lat'],0)))\n",
    "average_by_lat = daily_average_at_latitude.groupby('time-rounded-lat').agg(F.avg('avg(mean_temp)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time-rounded-lat: struct (nullable = false)\n",
      " |    |-- time-lat_time: timestamp (nullable = true)\n",
      " |    |-- col2: double (nullable = true)\n",
      " |-- avg(avg(mean_temp)): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we used the struct in order to do a simple groupby that we can flatten again to get information\n",
    "average_by_lat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temp: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- rounded-lat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the names weren't very descriptive, let's rework that\n",
    "average_by_lat = flatten_df(average_by_lat)\n",
    "average_by_lat = average_by_lat.withColumnRenamed('time-rounded-lat_col2', 'rounded-lat')\n",
    "average_by_lat = average_by_lat.withColumnRenamed('time-rounded-lat_time-lat_time', 'time')\n",
    "average_by_lat = average_by_lat.withColumnRenamed('avg(avg(mean_temp))', 'temp')\n",
    "average_by_lat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[time-rounded-lat#1176], functions=[avg(avg(mean_temp)#1162)])\n",
      "+- Exchange hashpartitioning(time-rounded-lat#1176, 200)\n",
      "   +- *(3) HashAggregate(keys=[time-rounded-lat#1176], functions=[partial_avg(avg(mean_temp)#1162)])\n",
      "      +- *(3) HashAggregate(keys=[time-lat#1150], functions=[avg(mean_temp#364)])\n",
      "         +- Exchange hashpartitioning(time-lat#1150, 200)\n",
      "            +- *(2) HashAggregate(keys=[time-lat#1150], functions=[partial_avg(mean_temp#364)])\n",
      "               +- *(2) Project [named_struct(time, time#134, lat, cast(lat#183 as double)) AS time-lat#1150, cast(mean_temp#77 as double) AS mean_temp#364]\n",
      "                  +- *(2) BroadcastHashJoin [station_number#72], [usaf#177], Inner, BuildRight\n",
      "                     :- *(2) Project [station_number#72, mean_temp#77, cast(concat(year#74, -, month#75, -, day#76) as timestamp) AS time#134]\n",
      "                     :  +- *(2) Filter isnotnull(station_number#72)\n",
      "                     :     +- *(2) FileScan parquet [station_number#72,year#74,month#75,day#76,mean_temp#77] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://w261-bucket/gsod/data.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(station_number)], ReadSchema: struct<station_number:string,year:string,month:string,day:string,mean_temp:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "                        +- *(1) Project [usaf#177, lat#183]\n",
      "                           +- *(1) Filter ((isnotnull(Country#180) && (Country#180 = US)) && isnotnull(usaf#177))\n",
      "                              +- *(1) FileScan csv [usaf#177,country#180,lat#183] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://w261-bucket/gsod/stations.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,US), IsNotNull(usaf)], ReadSchema: struct<usaf:string,country:string,lat:string>\n"
     ]
    }
   ],
   "source": [
    "# oh man that's a lot of stuff and since we can't cache the data this is taking forever to run.\n",
    "average_by_lat.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's avoid doing all that ^^ work multiple times.\n",
    "# Let's output to file and then read from that file to reduce our load for next time.\n",
    "average_by_lat.write.format(\"parquet\").save(\"gs://w261-bucket/gsod/average_by_lat.parquet\")\n",
    "average_by_lat_read = spark.read.parquet(\"gs://w261-bucket/gsod/average_by_lat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't time this, but it took about 17 minutes to save this to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [temp#1232,time#1233,rounded-lat#1234] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://w261-bucket/gsod/average_by_lat.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<temp:double,time:timestamp,rounded-lat:double>\n"
     ]
    }
   ],
   "source": [
    "average_by_lat_read.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------+\n",
      "|              temp|               time|rounded-lat|\n",
      "+------------------+-------------------+-----------+\n",
      "| 38.35000038146973|1940-02-06 00:00:00|       42.0|\n",
      "| 33.30499963760376|1940-02-26 00:00:00|       34.0|\n",
      "| 55.57837769791887|1940-04-14 00:00:00|       47.0|\n",
      "|  36.2037037037037|1940-12-04 00:00:00|       47.0|\n",
      "| 58.36562327047188|1941-10-05 00:00:00|       46.0|\n",
      "| 31.97604179879028|1941-12-09 00:00:00|       45.0|\n",
      "|23.598311814626058|1942-01-02 00:00:00|       39.0|\n",
      "| 33.36927065253258|1942-01-23 00:00:00|       45.0|\n",
      "|43.910237752823605|1942-03-23 00:00:00|       41.0|\n",
      "| 53.94781947502723|1942-10-25 00:00:00|       38.0|\n",
      "+------------------+-------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_by_lat_read.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418384"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small enough to fit in pandas for our final analysis. Let's do that. Pandas is really fast, \n",
    "# and if your dataset is small enough to run locally in pandas, do that!\n",
    "average_by_lat_read.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = average_by_lat_read.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>time</th>\n",
       "      <th>rounded-lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.350000</td>\n",
       "      <td>1940-02-06</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.305000</td>\n",
       "      <td>1940-02-26</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.578378</td>\n",
       "      <td>1940-04-14</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.203704</td>\n",
       "      <td>1940-12-04</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.365623</td>\n",
       "      <td>1941-10-05</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temp       time  rounded-lat\n",
       "0  38.350000 1940-02-06         42.0\n",
       "1  33.305000 1940-02-26         34.0\n",
       "2  55.578378 1940-04-14         47.0\n",
       "3  36.203704 1940-12-04         47.0\n",
       "4  58.365623 1941-10-05         46.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['rounded-lat','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rounded-lat</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-60.0</th>\n",
       "      <th>1943-08-01</th>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943-08-02</th>\n",
       "      <td>56.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943-08-03</th>\n",
       "      <td>60.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943-08-04</th>\n",
       "      <td>56.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943-08-05</th>\n",
       "      <td>57.700001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             temp\n",
       "rounded-lat time                 \n",
       "-60.0       1943-08-01  55.000000\n",
       "            1943-08-02  56.200001\n",
       "            1943-08-03  60.099998\n",
       "            1943-08-04  56.700001\n",
       "            1943-08-05  57.700001"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64Index([-60.0, -35.0, -32.0, -18.0, -10.0,   0.0,   4.0,   6.0,   9.0,\n",
      "               15.0,  17.0,  18.0,  19.0,  20.0,  21.0,  22.0,  23.0,  24.0,\n",
      "               25.0,  26.0,  27.0,  28.0,  29.0,  30.0,  31.0,  32.0,  33.0,\n",
      "               34.0,  35.0,  36.0,  37.0,  38.0,  39.0,  40.0,  41.0,  42.0,\n",
      "               43.0,  44.0,  45.0,  46.0,  47.0,  48.0,  49.0,  50.0,  51.0,\n",
      "               52.0,  53.0,  54.0,  55.0,  56.0,  57.0,  58.0,  59.0,  60.0,\n",
      "               61.0,  62.0,  63.0,  64.0,  65.0,  66.0,  67.0,  68.0,  69.0,\n",
      "               70.0,  71.0,  72.0,  80.0],\n",
      "             dtype='float64', name='rounded-lat')\n"
     ]
    }
   ],
   "source": [
    "lat_list = df.index.levels[0]\n",
    "print(lat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is temperature increasing? Data isn't very clean and we didn't perform any sensor corrections.\n",
    "%matplotlib notebook\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_roll = 365\n",
    "def f(x):\n",
    "    df.loc[x].rolling(n_roll).mean().plot()\n",
    "\n",
    "interact(f, x=lat_list);\n",
    "\n",
    "## Below is just a screenshot of the chart. To see the ineractive chart, follow the instructions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../screenshots/plot-screenshot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS:__\n",
    "* Why did we create a struct for our groupBy?\n",
    "* Why did we push our transformations to a file and load them again? - compare and contrast cache(), save to disk (checkpoint), saveManagedTable\n",
    "* Where could we have done this before to save computation time?\n",
    "* Why did we do a rolling average of temperature?\n",
    "* Isn't pandas a lot easier to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__INSTRUCTOR TALKING POINTS__\n",
    "* Why did we create a struct for our groupBy?\n",
    "> This allowed us to create a composite key that we can access the original keys in a seamless fashion\n",
    "\n",
    "* Why did we push our transformations to a file and load them again?\n",
    "> This offload allowed us to have non-repeated computation once we went through the transformation steps. \n",
    "\n",
    "* Where could we have done this before to save computation time?\n",
    "> We could have done a similar file offload prior to generating our histograms that repeated the computation 3x.\n",
    "\n",
    "* Why did we do a rolling average of the temprature?\n",
    "> We only care about the long term change in temperature and the day to day variations are dependent on various weather effects such as rain or clouds.\n",
    "\n",
    "* Isn't pandas a lot easier to use?\n",
    "> Yes, but it also has significant limitations in comparison to the raw data we're able to process with Spark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save this df for later plotting, just in case. We're still in the cloud here.\n",
    "df.to_csv(\"pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now download the file to local machine\n",
    "!gcloud compute scp w261-demo-m:/pandas.csv ~/MyDocuments/UCB/w261/Instructors/LiveSessionMaterials/wk08Demo_DataFrames/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP THIS unless you want to render the chart while working locally:\n",
    "IMPORTANT: We are now on the local machine. Jupyter lab does not support matplotlib notebook. To view the chart, open in jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rounded-lat</th>\n",
       "      <th>time</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-60.0</td>\n",
       "      <td>1943-08-01</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-60.0</td>\n",
       "      <td>1943-08-02</td>\n",
       "      <td>56.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-60.0</td>\n",
       "      <td>1943-08-03</td>\n",
       "      <td>60.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-60.0</td>\n",
       "      <td>1943-08-04</td>\n",
       "      <td>56.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-60.0</td>\n",
       "      <td>1943-08-05</td>\n",
       "      <td>57.700001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rounded-lat        time       temp\n",
       "0        -60.0  1943-08-01  55.000000\n",
       "1        -60.0  1943-08-02  56.200001\n",
       "2        -60.0  1943-08-03  60.099998\n",
       "3        -60.0  1943-08-04  56.700001\n",
       "4        -60.0  1943-08-05  57.700001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['rounded-lat','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64Index([-60.0, -35.0, -32.0, -18.0, -10.0,   0.0,   4.0,   6.0,   9.0,\n",
      "               15.0,  17.0,  18.0,  19.0,  20.0,  21.0,  22.0,  23.0,  24.0,\n",
      "               25.0,  26.0,  27.0,  28.0,  29.0,  30.0,  31.0,  32.0,  33.0,\n",
      "               34.0,  35.0,  36.0,  37.0,  38.0,  39.0,  40.0,  41.0,  42.0,\n",
      "               43.0,  44.0,  45.0,  46.0,  47.0,  48.0,  49.0,  50.0,  51.0,\n",
      "               52.0,  53.0,  54.0,  55.0,  56.0,  57.0,  58.0,  59.0,  60.0,\n",
      "               61.0,  62.0,  63.0,  64.0,  65.0,  66.0,  67.0,  68.0,  69.0,\n",
      "               70.0,  71.0,  72.0,  80.0],\n",
      "             dtype='float64', name='rounded-lat')\n"
     ]
    }
   ],
   "source": [
    "lat_list = df.index.levels[0]\n",
    "print(lat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989bc3a3853d49c1bb3582bb5649bf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=(-60.0, -35.0, -32.0, -18.0, -10.0, 0.0, 4.0, 6.0, 9.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Is temperature increasing? Data isn't very clean and we didn't perform any sensor corrections.\n",
    "%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_roll = 365\n",
    "def f(x):\n",
    "    df.loc[x].rolling(n_roll).mean().plot()\n",
    "\n",
    "interact(f, x=lat_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "462px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "567px",
    "left": "0px",
    "right": "707.4456787109375px",
    "top": "105px",
    "width": "243px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
